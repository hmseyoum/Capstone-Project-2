{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spanish_english_translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z579-ISl9Zj6"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT20LFmb3jSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a254d1-62e2-4b73-ec0c-cad0917c47ec"
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'torch.__version__: {torch.__version__}')\n",
        "print(f'torch device: {device}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.__version__: 1.7.0+cu101\n",
            "torch device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuXJjo9NuT8"
      },
      "source": [
        "## Import Data from Google Drive\n",
        "I stored the data on my Google Drive, but you can also obtain it from [here](http://www.manythings.org/anki/) as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Ox1YURzVhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8774835d-3084-4c60-d2fd-99b59e0c9e01"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD8Qy0eC0ZtA"
      },
      "source": [
        "f = open('/gdrive/My Drive/Data Science/Capstone Project 2/spa.txt', encoding='UTF-8').read().strip().split('\\n')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mVlB0W14b4G"
      },
      "source": [
        "lines = f"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JouLb6Eo4f28"
      },
      "source": [
        "# sample size (try with smaller sample size to reduce computation)\n",
        "num_examples = 30000 \n",
        "\n",
        "# creates lists containing each pair\n",
        "original_word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as2-5vGn4jUa"
      },
      "source": [
        "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"es\", \"info\"])\n",
        "data = data.drop(columns=\"info\", axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913VSLih4lY3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "648a8b59-830f-4b8b-c0e1-1f62a9fc2440"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>es</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Ve.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Vete.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Vaya.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Váyase.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Hola.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   eng       es\n",
              "0  Go.      Ve.\n",
              "1  Go.    Vete.\n",
              "2  Go.    Vaya.\n",
              "3  Go.  Váyase.\n",
              "4  Hi.    Hola."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCUSf31E4m6t"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2pLaZkNqrv"
      },
      "source": [
        "## Data Exploration\n",
        "Let's explore the dataset a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLV4RCR4pXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b870536d-09e1-4ece-ef72-9fa0b9f335af"
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"es\"] = data.es.apply(lambda w: preprocess_sentence(w))\n",
        "data.sample(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>es</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5095</th>\n",
              "      <td>&lt;start&gt; look in there . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; mira alli . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27607</th>\n",
              "      <td>&lt;start&gt; where shall we begin ? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ¿ por donde deberiamos comenzar ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13207</th>\n",
              "      <td>&lt;start&gt; you re impatient . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; eres impaciente . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24329</th>\n",
              "      <td>&lt;start&gt; don t try to hide it . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; no intentes ocultarlo . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3420</th>\n",
              "      <td>&lt;start&gt; she is wrong . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ella esta equivocada . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27328</th>\n",
              "      <td>&lt;start&gt; we elected her mayor . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; nosotros la elegimos alcalde . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23310</th>\n",
              "      <td>&lt;start&gt; tom wasn t babbling . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; tom no estaba farfullando . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22415</th>\n",
              "      <td>&lt;start&gt; she studies english . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ella estudia ingles . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10935</th>\n",
              "      <td>&lt;start&gt; how you ve grown ! &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; como has crecido ! &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>&lt;start&gt; who ll go ? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ¿ quien ira ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        eng                                               es\n",
              "5095          <start> look in there . <end>                        <start> mira alli . <end>\n",
              "27607  <start> where shall we begin ? <end>  <start> ¿ por donde deberiamos comenzar ? <end>\n",
              "13207      <start> you re impatient . <end>                  <start> eres impaciente . <end>\n",
              "24329  <start> don t try to hide it . <end>            <start> no intentes ocultarlo . <end>\n",
              "3420           <start> she is wrong . <end>             <start> ella esta equivocada . <end>\n",
              "27328  <start> we elected her mayor . <end>     <start> nosotros la elegimos alcalde . <end>\n",
              "23310   <start> tom wasn t babbling . <end>        <start> tom no estaba farfullando . <end>\n",
              "22415   <start> she studies english . <end>              <start> ella estudia ingles . <end>\n",
              "10935      <start> how you ve grown ! <end>                 <start> como has crecido ! <end>\n",
              "978               <start> who ll go ? <end>                      <start> ¿ quien ira ? <end>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqM7ZncM8V9B"
      },
      "source": [
        "## Building Vocabulary Index\n",
        "The class below is useful for creating the vocabular and index mappings which will be used to convert out inputs into indexed sequences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rXA7-N34sok"
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        \n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fesymsn34v7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01919915-6520-4eab-df85-4f5fbcd74a04"
      },
      "source": [
        "# index language using the class above\n",
        "inp_index = LanguageIndex(data[\"es\"].values.tolist())\n",
        "targ_index = LanguageIndex(data[\"eng\"].values.tolist())\n",
        "# Vectorize the input and target languages\n",
        "input_tensor = [[inp_index.word2idx[s] for s in es.split(' ')]  for es in data[\"es\"].values.tolist()]\n",
        "target_tensor = [[targ_index.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]\n",
        "input_tensor[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 9009, 3, 4],\n",
              " [5, 9128, 3, 4],\n",
              " [5, 9001, 3, 4],\n",
              " [5, 9008, 3, 4],\n",
              " [5, 4670, 3, 4],\n",
              " [5, 2267, 1, 4],\n",
              " [5, 2265, 1, 4],\n",
              " [5, 2264, 1, 4],\n",
              " [5, 2272, 1, 4],\n",
              " [5, 2272, 3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPordlA-N4qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23616ab1-8225-44b8-8ea2-d02482bd39f8"
      },
      "source": [
        "target_tensor[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 1810, 3, 4],\n",
              " [5, 1810, 3, 4],\n",
              " [5, 1810, 3, 4],\n",
              " [5, 1810, 3, 4],\n",
              " [5, 2009, 3, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 1, 4],\n",
              " [5, 3574, 3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cwX-0rt4zmN"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycYy5gq641Uy"
      },
      "source": [
        "# calculate the max_length of input and output tensor\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q05E5IwH42_1"
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    padded = np.zeros((max_len), dtype=np.int64)\n",
        "    if len(x) > max_len: padded[:] = x[:max_len]\n",
        "    else: padded[:len(x)] = x\n",
        "    return padded"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66dJPqzV44jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1432cd1d-2806-44e0-9806-9c5217ae8078"
      },
      "source": [
        "# inplace padding\n",
        "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
        "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
        "len(target_tensor)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvatfCWS46T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0747d1d-b97c-48e4-de23-d6506fb5f5ca"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFO3obpOsoB"
      },
      "source": [
        "## Load data into DataLoader for Batching\n",
        "This is just preparing the dataset so that it can be efficiently fed into the model through batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRQKwxf479Q"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDSxA4OM5Qlp"
      },
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2WukeVF8NVn"
      },
      "source": [
        "## Parameters\n",
        "Let's define the hyperparameters and other things we need for training our NMT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Be7lOZ5R-d"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_index.word2idx)\n",
        "vocab_tar_size = len(targ_index.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blYXo7pv5TOu"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrsQ7dTg5V__"
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X1h155CPQ1Y"
      },
      "source": [
        "## Testing the Encoder\n",
        "Before proceeding with training, we should always try to test out model behavior such as the size of outputs just to make that things are going as expected. In PyTorch this can be done easily since everything comes in eager execution by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbSLACY45Xz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06eae917-0443-4957-8cde-6ca2236d70d7"
      },
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([9, 64, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiMRxHQFGPtt"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input word is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
        "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "* `merged vector = concat(embedding output, context vector)`\n",
        "* This merged vector is then given to the GRU\n",
        "  \n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4djvgil5bMQ"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsG5We7Sk_UR"
      },
      "source": [
        "## Testing the Decoder\n",
        "Similarily, try to test the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmipPRVx5fqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903cdb26-6af8-45c6-eda9-b7e65e52ae58"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[targ_index.word2idx['<start>']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([64, 16])\n",
            "Output:  torch.Size([64, 11])\n",
            "Encoder Output:  torch.Size([11, 64, 1024])\n",
            "Encoder Hidden:  torch.Size([1, 64, 1024])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 4815])\n",
            "Decoder Hidden:  torch.Size([1, 64, 1024])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QclyWIop5dRG"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjMMYJv85hVT"
      },
      "source": [
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6_WoDZM7reU"
      },
      "source": [
        "## Training\n",
        "Now we start the training. We are only using 10 epochs but you can expand this to keep trainining the model for a longer period of time. Note that in this case we are teacher forcing during training. Find a more detailed explanation in the official TensorFlow [implementation](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) of this notebook provided by the TensorFlow team. \n",
        "\n",
        "- Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
        "- The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
        "- The decoder returns the predictions and the decoder hidden state.\n",
        "- The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "- Use teacher forcing to decide the next input to the decoder.\n",
        "- Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
        "- The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN8G-3YY8ADm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dd5b24-2700-48ad-e76e-3587388fda04"
      },
      "source": [
        "EPOCHS = 2\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[targ_index.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5914\n",
            "Epoch 1 Batch 100 Loss 1.6569\n",
            "Epoch 1 Batch 200 Loss 1.2886\n",
            "Epoch 1 Batch 300 Loss 1.1354\n",
            "Epoch 1 Loss 1.4750\n",
            "Time taken for 1 epoch 20.449249505996704 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8190\n",
            "Epoch 2 Batch 100 Loss 0.7676\n",
            "Epoch 2 Batch 200 Loss 0.6161\n",
            "Epoch 2 Batch 300 Loss 0.8089\n",
            "Epoch 2 Loss 0.7294\n",
            "Time taken for 1 epoch 20.496861457824707 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XThy6IQhAnLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af8a83e-fd99-4691-e384-492e1a3922f4"
      },
      "source": [
        "print(' '.join([targ_index.idx2word[i] for i in target_tensor[10000]]))\n",
        "print(' '.join([inp_index.idx2word[i] for i in input_tensor[10000]]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> we re not crazy . <end> <pad> <pad> <pad> <pad>\n",
            "<start> no estamos locos . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn_UZUsfAnAE"
      },
      "source": [
        "for (inp, targ, inp_len) in dataset:\n",
        "    break"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZJlxvpAsXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b5bc3a-023e-4d60-9e34-77556837d1e9"
      },
      "source": [
        "print(inp)\n",
        "print(targ)\n",
        "print(inp_len)\n",
        "#xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "#enc_output, enc_hidden = encoder(xs.to(device), lens, device=device)\n",
        "#dec_hidden = enc_hidden"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   5, 3655, 8921,  ...,    0,    0,    0],\n",
            "        [   5, 9334, 5697,  ...,    0,    0,    0],\n",
            "        [   5, 8702, 9309,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   5, 3267, 8630,  ...,    0,    0,    0],\n",
            "        [   5, 3873, 3371,  ...,    0,    0,    0],\n",
            "        [   5,  824, 3655,  ...,    0,    0,    0]])\n",
            "tensor([[   5, 1961, 3584,    7, 2030,    3,    4,    0,    0,    0,    0],\n",
            "        [   5,  205, 4805, 2151, 2603,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 2124, 3624, 4362, 2914, 2695,    3,    4,    0,    0,    0],\n",
            "        [   5, 2269, 3584, 2758,  555, 3584,    3,    4,    0,    0,    0],\n",
            "        [   5, 2124, 4552, 1903, 1400,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2124, 1139, 4186, 3679, 4805,    3,    4,    0,    0,    0],\n",
            "        [   5, 3736, 3584, 4081,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 4612, 2916, 2456, 4277,    3,    4,    0,    0,    0],\n",
            "        [   5, 4362, 4097,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 1329, 3926,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 4643,  353, 4278, 2569, 4530,    3,    4,    0,    0,    0],\n",
            "        [   5, 1212, 4643, 1957, 4339,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 4805, 3371, 3733,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5,  278, 2264, 2000,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 4277, 2490, 1212,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 4288, 3060, 4362, 4671,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2269, 4621,  115, 2758, 1546,    3,    4,    0,    0,    0],\n",
            "        [   5, 1961, 2264,    7, 4290,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2826, 2916, 3685, 4352,  657,    3,    4,    0,    0,    0],\n",
            "        [   5, 1961, 2264, 1782, 2909,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4681, 3584, 1819, 2914,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 4362, 2480,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 4288,  622, 2171,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 4286,  205, 2826, 1817,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2124,  127, 2456,  612,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2124, 2351, 4277,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 1961, 4672,  603,  411,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1780, 2603, 4530,  254, 1349,    3,    4,    0,    0,    0],\n",
            "        [   5, 2124,  628, 4186, 1477, 2269,    3,    4,    0,    0,    0],\n",
            "        [   5,  842, 2171,    3, 2124, 2208,    3,    4,    0,    0,    0],\n",
            "        [   5, 3736, 3563, 1999, 1491,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4702,  205, 4805,  115, 3589,    6,    4,    0,    0,    0],\n",
            "        [   5, 2124, 1903, 4352, 3451,    7,  650,    3,    4,    0,    0],\n",
            "        [   5, 2124, 2549, 2847, 1197,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2433, 3584,  238, 4362,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4362,  197,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 1227, 4186, 2456, 3986,    3,    4,    0,    0,    0],\n",
            "        [   5, 2124, 2549, 3805,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 1407, 2023,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124,  994,    7, 2521,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1227, 4186, 4198,    1,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 2529, 2430, 3944,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4362, 3584, 4050,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 3951, 3859,    2, 3160,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4362, 2264, 2898,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 2456, 4808, 2961,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2414, 2000,  254, 2915,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2567,  976,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 2269, 4621, 4557, 4534,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 3160, 1612, 4304,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 4621, 2269,    7, 1254,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 2124, 2549,  330,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 2549, 4370, 2909, 1664, 4805,    3,    4,    0,    0],\n",
            "        [   5, 4643,  628, 4186, 4022, 2000,    3,    4,    0,    0,    0],\n",
            "        [   5, 4702,  205, 4805,  115, 2000,    6,    4,    0,    0,    0],\n",
            "        [   5, 2124, 2549, 1127,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2433, 3584, 3331,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 2124, 4612, 4805, 4352, 2479,    3,    4,    0,    0,    0],\n",
            "        [   5, 4681,  548, 4805, 2000,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 4805, 2520,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 3625,  732,    3,    4,    0,    0,    0,    0,    0,    0],\n",
            "        [   5, 1961, 1945,    7,  625,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2124, 2549, 2171, 2529,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4277, 3584, 2445,    3,    4,    0,    0,    0,    0,    0]])\n",
            "tensor([ 6,  7, 10,  8,  4,  6,  6,  6,  5,  5,  9,  6,  5,  7,  6,  9,  7,  6,\n",
            "         8,  7,  7,  5,  5,  7, 10,  5,  6,  7,  6,  6,  8,  9,  8,  6,  6,  6,\n",
            "         8,  5,  5,  5,  5,  9,  6,  9,  6,  7,  8,  5,  6,  8,  7,  5,  8,  7,\n",
            "         9,  5,  4,  6,  9,  5,  5,  7,  5,  7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2aUneQAAsOw"
      },
      "source": [
        "batch_size = 64\n",
        "def spanish_to_english(spanish_sentence):\n",
        "    spanish_tensor = [inp_index.word2idx[w] for w in spanish_sentence.split()]\n",
        "    \n",
        "    lens = torch.tensor([len(spanish_tensor)] * batch_size)\n",
        "    lens = lens.to(device)\n",
        "\n",
        "    x = torch.tensor([list(spanish_tensor)] * batch_size)\n",
        "    x = x.to(device)\n",
        "\n",
        "    # xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "    enc_output, enc_hidden = encoder(x=x, lens=lens.cpu(), device=device)\n",
        "    \n",
        "    # encode(spanish_sentence)\n",
        "    return enc_output"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qFea_16AsLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "c5058d31-cb06-4a0c-c6e5-8abe8d91452c"
      },
      "source": [
        "spanish_to_english('<start> como estas usted ? <end>')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-ccaff8440599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspanish_to_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<start> como estas usted ? <end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-95a34c3209d7>\u001b[0m in \u001b[0;36mspanish_to_english\u001b[0;34m(spanish_sentence)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# xs, ys, lens = sort_batch(inp, targ, inp_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# encode(spanish_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-8f502332a23a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lens, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# x transformed = max_len X batch_size X embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# x = x.permute(1,0,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected `len(lengths)` to be equal to batch_size, but got 64 (batch_size=6)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tF5jMP0-cmv"
      },
      "source": [
        "## Final Words\n",
        "Notice that we only trained the model and that's it. In fact, this notebook is in experimental phase, so there could also be some bugs or something I missed during the process of converting code or training. Please comment your concerns here or submit it as an issue in the [GitHub version](https://github.com/omarsar/pytorch_neural_machine_translation_attention) of this notebook. I will appreciate it!\n",
        "\n",
        "We didn't evaluate the model or analyzed it. To encourage you to practice what you have learned in the notebook, I will suggest that you try to convert the TensorFlow code used in the [original notebook](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) and complete this notebook. I believe the code should be straightforward, the hard part was already done in this notebook. If you manage to complete it, please submit a PR on the GitHub version of this notebook. I will gladly accept your PR. Thanks for reading and hope this notebook was useful. Keep tuned for notebooks like this on my Twitter ([omarsar0](https://twitter.com/omarsar0)). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl4ZgMd-KyTU"
      },
      "source": [
        "## References\n",
        "\n",
        "### Seq2Seq:\n",
        "  - Sutskever et al. (2014) - [Sequence to Sequence Learning with Neural Networks](Sequence to Sequence Learning with Neural Networks)\n",
        "  - [Sequence to sequence model: Introduction and concepts](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)\n",
        "  - [Blog on seq2seq](https://guillaumegenthial.github.io/sequence-to-sequence.html)\n",
        "  - [Bahdanau et al. (2016) NMT jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "  - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
      ]
    }
  ]
}