{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eng_french_translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG1YOyDEsX5a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0yMbQZPsoI9"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jV4I7Pisq4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2830dad8-6a5a-435b-88d4-a33cf307528c"
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'torch.__version__: {torch.__version__}')\n",
        "print(f'torch device: {device}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.__version__: 1.7.0+cu101\n",
            "torch device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6NjY6Lqtqx9",
        "outputId": "cc2786f7-066b-48c7-b2bb-c2bd777c276c"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIYu5gpGs9Dv"
      },
      "source": [
        "## Import Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlgmDp0Ns55c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194bf2c7-6fe5-4757-c492-485f2f1540f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F69_0zY1tERu"
      },
      "source": [
        "f = open('/gdrive/My Drive/Data Science/Capstone Project 2/fra.txt', encoding='UTF-8').read().strip().split('\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MVyInhDtKBR"
      },
      "source": [
        "lines = f"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmrjWXZtO2h"
      },
      "source": [
        "# sample size (try with smaller sample size to reduce computation)\n",
        "num_examples = 30000 \n",
        "\n",
        "# creates lists containing each pair\n",
        "original_word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzoewY_tSn8"
      },
      "source": [
        "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"fr\", \"info\"])\n",
        "data = data.drop(columns=\"info\", axis=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXJpqDpQtVCN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8390f4d6-d1ae-44b4-f72f-1808a9a1831c"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    eng        fr\n",
              "0   Go.      Va !\n",
              "1   Hi.   Salut !\n",
              "2   Hi.    Salut.\n",
              "3  Run!   Cours !\n",
              "4  Run!  Courez !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evVoWThqtZ_p"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulYQCfcatcup"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-DVWfcJtZ4m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "ecd23a77-7d77-4f95-d446-8be64861f435"
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"fr\"] = data.fr.apply(lambda w: preprocess_sentence(w))\n",
        "data.sample(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29203</th>\n",
              "      <td>&lt;start&gt; he s got a headache . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; il a mal a la tete . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17395</th>\n",
              "      <td>&lt;start&gt; what s your name ? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; comment tu t appelles ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21100</th>\n",
              "      <td>&lt;start&gt; the bath is ready . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; le bain est pret . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9899</th>\n",
              "      <td>&lt;start&gt; you re awesome . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; tu es geniale . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18581</th>\n",
              "      <td>&lt;start&gt; he can t help you . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; il ne peut pas vous aider . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25269</th>\n",
              "      <td>&lt;start&gt; is this seat empty ? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; cette place est elle libre ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21848</th>\n",
              "      <td>&lt;start&gt; we have some time . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; nous avons un peu de temps . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25332</th>\n",
              "      <td>&lt;start&gt; it started to snow . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; il commenca a neiger . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16146</th>\n",
              "      <td>&lt;start&gt; please don t ask . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ne demande pas , je te prie ! &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22656</th>\n",
              "      <td>&lt;start&gt; you re contagious . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; tu es contagieuse . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       eng                                           fr\n",
              "29203  <start> he s got a headache . <end>           <start> il a mal a la tete . <end>\n",
              "17395     <start> what s your name ? <end>        <start> comment tu t appelles ? <end>\n",
              "21100    <start> the bath is ready . <end>             <start> le bain est pret . <end>\n",
              "9899        <start> you re awesome . <end>                <start> tu es geniale . <end>\n",
              "18581    <start> he can t help you . <end>    <start> il ne peut pas vous aider . <end>\n",
              "25269   <start> is this seat empty ? <end>   <start> cette place est elle libre ? <end>\n",
              "21848    <start> we have some time . <end>   <start> nous avons un peu de temps . <end>\n",
              "25332   <start> it started to snow . <end>         <start> il commenca a neiger . <end>\n",
              "16146     <start> please don t ask . <end>  <start> ne demande pas , je te prie ! <end>\n",
              "22656    <start> you re contagious . <end>            <start> tu es contagieuse . <end>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMUJXwbFuczX"
      },
      "source": [
        "## Building Vocabulary Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Gc2d__tZxg"
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        \n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ZurvUutZrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99400c3d-f605-4182-9712-874da2d847f5"
      },
      "source": [
        "# index language using the class above\n",
        "inp_lang = LanguageIndex(data[\"fr\"].values.tolist())\n",
        "targ_lang = LanguageIndex(data[\"eng\"].values.tolist())\n",
        "# Vectorize the input and target languages\n",
        "input_tensor = [[inp_lang.word2idx[s] for s in fr.split(' ')]  for fr in data[\"fr\"].values.tolist()]\n",
        "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]\n",
        "input_tensor[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 7313, 1, 4],\n",
              " [5, 6391, 1, 4],\n",
              " [5, 6391, 3, 4],\n",
              " [5, 1674, 1, 4],\n",
              " [5, 1667, 1, 4],\n",
              " [5, 5768, 6, 4],\n",
              " [5, 986, 275, 1, 4],\n",
              " [5, 589, 3088, 1, 4],\n",
              " [5, 7, 4065, 176, 1, 4],\n",
              " [5, 6427, 3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMpfGx8fulDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6984fea8-21db-49f9-ce33-ecb6490f2ca1"
      },
      "source": [
        "target_tensor[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 1629, 3, 4],\n",
              " [5, 1821, 3, 4],\n",
              " [5, 1821, 3, 4],\n",
              " [5, 3221, 1, 4],\n",
              " [5, 3221, 1, 4],\n",
              " [5, 4259, 6, 4],\n",
              " [5, 4331, 1, 4],\n",
              " [5, 1419, 1, 4],\n",
              " [5, 1806, 1, 4],\n",
              " [5, 2099, 3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWmeBbhjjnEq",
        "outputId": "3ea8b487-2078-461a-9069-778a4cff771f"
      },
      "source": [
        "list(inp_lang.idx2word.values())[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " '!',\n",
              " ',',\n",
              " '.',\n",
              " '<end>',\n",
              " '<start>',\n",
              " '?',\n",
              " 'a',\n",
              " 'abandonna',\n",
              " 'abandonne']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc-PigCgjoZn",
        "outputId": "97457486-f444-4b3d-987d-f7ddbdc9c3e5"
      },
      "source": [
        "list(inp_lang.word2idx.keys())[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " '!',\n",
              " ',',\n",
              " '.',\n",
              " '<end>',\n",
              " '<start>',\n",
              " '?',\n",
              " 'a',\n",
              " 'abandonna',\n",
              " 'abandonne']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klmi3gS7jofW",
        "outputId": "1b35eef4-bdbd-4ccb-ac01-15f4e8337453"
      },
      "source": [
        "list(inp_lang.word2idx.keys())[1350:1450]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cocufiee',\n",
              " 'coffre',\n",
              " 'cogna',\n",
              " 'cogne',\n",
              " 'cogner',\n",
              " 'coiffe',\n",
              " 'coiffee',\n",
              " 'coin',\n",
              " 'coince',\n",
              " 'coincee',\n",
              " 'coincees',\n",
              " 'coinces',\n",
              " 'coincidence',\n",
              " 'colere',\n",
              " 'collant',\n",
              " 'collation',\n",
              " 'colle',\n",
              " 'collectif',\n",
              " 'collectionne',\n",
              " 'collegue',\n",
              " 'coller',\n",
              " 'colocataire',\n",
              " 'coma',\n",
              " 'combat',\n",
              " 'combattre',\n",
              " 'combattrons',\n",
              " 'combien',\n",
              " 'comedien',\n",
              " 'comediens',\n",
              " 'comedies',\n",
              " 'commande',\n",
              " 'commanderai',\n",
              " 'commandes',\n",
              " 'comme',\n",
              " 'commenca',\n",
              " 'commencames',\n",
              " 'commence',\n",
              " 'commencement',\n",
              " 'commencer',\n",
              " 'commencerai',\n",
              " 'commencerons',\n",
              " 'commences',\n",
              " 'commencez',\n",
              " 'commencons',\n",
              " 'comment',\n",
              " 'commentaire',\n",
              " 'commere',\n",
              " 'commettons',\n",
              " 'commis',\n",
              " 'commises',\n",
              " 'compagnie',\n",
              " 'compagnon',\n",
              " 'compare',\n",
              " 'compassion',\n",
              " 'compatis',\n",
              " 'competitif',\n",
              " 'completement',\n",
              " 'complexe',\n",
              " 'compliment',\n",
              " 'complique',\n",
              " 'complot',\n",
              " 'comporte',\n",
              " 'comportez',\n",
              " 'composer',\n",
              " 'comprehensible',\n",
              " 'comprenait',\n",
              " 'comprend',\n",
              " 'comprendra',\n",
              " 'comprendre',\n",
              " 'comprendront',\n",
              " 'comprends',\n",
              " 'comprenez',\n",
              " 'comprenons',\n",
              " 'compris',\n",
              " 'comprit',\n",
              " 'compromis',\n",
              " 'comptable',\n",
              " 'compte',\n",
              " 'compter',\n",
              " 'compteur',\n",
              " 'comptez',\n",
              " 'comptons',\n",
              " 'con',\n",
              " 'concentration',\n",
              " 'concentre',\n",
              " 'concentrer',\n",
              " 'concentres',\n",
              " 'concentrez',\n",
              " 'concentrons',\n",
              " 'concert',\n",
              " 'concevable',\n",
              " 'conclu',\n",
              " 'conclue',\n",
              " 'concocta',\n",
              " 'concouru',\n",
              " 'concret',\n",
              " 'concu',\n",
              " 'concue',\n",
              " 'condamne',\n",
              " 'condamnee']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD1MQ2egulCk"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbualJyxuk-C"
      },
      "source": [
        "# calculate the max_length of input and output tensor\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N41-JI_Huk89"
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    padded = np.zeros((max_len), dtype=np.int64)\n",
        "    if len(x) > max_len: \n",
        "        padded[:] = x[:max_len]\n",
        "    else: \n",
        "        padded[:len(x)] = x\n",
        "    return padded"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yvSd1jouk4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb8c99e-13dc-40e2-c2d4-dab4e7976d7e"
      },
      "source": [
        "# inplace padding\n",
        "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
        "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
        "len(target_tensor)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6nevUmHuk3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e36c70-97b2-4497-e2a7-a09efe3848c5"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS3_CUIRvTm1"
      },
      "source": [
        "## Load Data into DataLoader for Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bvgd0aVukyd"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiZaAXSQukvH"
      },
      "source": [
        "# convert the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZ45AG5vy2n"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dodiMk_gvysD"
      },
      "source": [
        "Defining hyperparameters and other things we'll need to train NMT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGnG7T9vya2"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnpF29dXvyWq"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_H4sS2vukrx"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvfXljsRtZkR"
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dexdlpHt9SoT"
      },
      "source": [
        "## Testing the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZwW4LUetZZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f57cae-5a99-495a-dbe2-713499a8260c"
      },
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 64, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMRveYja9L29"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enao9Cm-AfNP"
      },
      "source": [
        "## Testing the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIWAOW_6AYSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1ea88d-8ac5-47ac-d958-77348084af68"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([64, 17])\n",
            "Output:  torch.Size([64, 10])\n",
            "Encoder Output:  torch.Size([11, 64, 1024])\n",
            "Encoder Hidden:  torch.Size([1, 64, 1024])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 4367])\n",
            "Decoder Hidden:  torch.Size([1, 64, 1024])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuzJAJS9jKF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c91e48-a511-432e-fd46-27661d694f09"
      },
      "source": [
        "lens.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOU6E-WrjLc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973b1c9a-4dda-4dd6-f504-3272b506d850"
      },
      "source": [
        "xs.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([17, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPUgL-ijLlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbca73a-5eac-4412-a770-5032cedf3fa9"
      },
      "source": [
        "ys.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iMv5wZsAYOA"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMPEx7t9L0x"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6bESITNBNPw"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlbbLAne9Lw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9978f3d3-a356-4efb-80f7-529a7e28e7b0"
      },
      "source": [
        "EPOCHS = 2\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.8987\n",
            "Epoch 1 Batch 100 Loss 1.5722\n",
            "Epoch 1 Batch 200 Loss 1.1926\n",
            "Epoch 1 Batch 300 Loss 1.0822\n",
            "Epoch 1 Loss 1.4488\n",
            "Time taken for 1 epoch 19.529693841934204 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7467\n",
            "Epoch 2 Batch 100 Loss 0.7385\n",
            "Epoch 2 Batch 200 Loss 0.5799\n",
            "Epoch 2 Batch 300 Loss 0.5989\n",
            "Epoch 2 Loss 0.6919\n",
            "Time taken for 1 epoch 19.766449213027954 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-j_0cj-9LvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bf24b3-383f-4d4d-eb1c-22fc6e3f057c"
      },
      "source": [
        "print(' '.join([targ_lang.idx2word[i] for i in target_tensor[10000]]))\n",
        "print(' '.join([inp_lang.idx2word[i] for i in input_tensor[10000]]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> you re welcome . <end> <pad> <pad> <pad> <pad>\n",
            "<start> de rien ! <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1vQxpXX9Ltf"
      },
      "source": [
        "for (inp, targ, inp_len) in dataset:\n",
        "    break"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhKk1y09Loy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7fcb14-57ba-4f97-a4bd-410cb26a4ebe"
      },
      "source": [
        "print(inp)\n",
        "print(targ)\n",
        "print(inp_len)\n",
        "#xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "#enc_output, enc_hidden = encoder(xs.to(device), lens, device=device)\n",
        "#dec_hidden = enc_hidden"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   5, 3973, 4302,  ...,    0,    0,    0],\n",
            "        [   5,  985, 2803,  ...,    0,    0,    0],\n",
            "        [   5, 7264, 4237,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   5,  985, 2803,  ...,    0,    0,    0],\n",
            "        [   5, 7084, 4237,  ...,    0,    0,    0],\n",
            "        [   5, 7264,  494,  ...,    0,    0,    0]])\n",
            "tensor([[   5, 1922, 1202,    7, 2281,    3,    4,    0,    0,    0],\n",
            "        [   5, 3887, 2044, 1894,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4357,  130, 2353,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 2458, 4357,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 3042, 1909,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 3940, 2044, 3236,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 2518,    7, 2814,    3,    4,    0,    0,    0],\n",
            "        [   5, 2190, 3865,  109,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1892,   14, 3942,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 4210, 3273, 3545,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 3866,  731, 3652,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 2303, 1637, 3930, 2190,    3,    4,    0,    0],\n",
            "        [   5, 1922, 2303, 4021,    3,    4,    0,    0,    0,    0],\n",
            "        [   5,  179, 4357, 1750,    6,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 1037, 2049, 2622,    3,    4,    0,    0,    0],\n",
            "        [   5, 1922, 2303, 2562,    7,  344,    3,    4,    0,    0],\n",
            "        [   5,  321, 2852,    2,  540, 1424,    3,    4,    0,    0],\n",
            "        [   5, 4210, 1137,  103, 2542,    3,    4,    0,    0,    0],\n",
            "        [   5, 1892,  376, 4357,  179,    1,    4,    0,    0,    0],\n",
            "        [   5, 1922, 2303, 2331,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 2253, 1103,  153,    3,    4,    0,    0,    0],\n",
            "        [   5, 3865, 3232, 1924,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4244, 3232, 1969, 3866,  276,    6,    4,    0,    0],\n",
            "        [   5, 3940, 2230, 3768,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 2518,    7, 2409,    3,    4,    0,    0,    0],\n",
            "        [   5, 1922, 2583, 3866, 3218,    3,    4,    0,    0,    0],\n",
            "        [   5, 3940, 2044, 2227,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 3574, 4360, 2403,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 3940, 3359, 3866, 3782,    3,    4,    0,    0,    0],\n",
            "        [   5, 1603, 2654, 2602, 3866, 4208,    3,    4,    0,    0],\n",
            "        [   5, 1922, 1767, 4357,  452,    3,    4,    0,    0,    0],\n",
            "        [   5, 3865,  570, 1744,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4259, 2306, 3887, 2814,    6,    4,    0,    0,    0],\n",
            "        [   5, 4261, 1113, 3781, 4357, 3440,    6,    4,    0,    0],\n",
            "        [   5, 1922, 1872, 3887, 4317,    3,    4,    0,    0,    0],\n",
            "        [   5, 1922, 2303,  354,  598,    3,    4,    0,    0,    0],\n",
            "        [   5, 3866,  749, 2044,  753,    3,    4,    0,    0,    0],\n",
            "        [   5, 3786,  596,    3,    4,    0,    0,    0,    0,    0],\n",
            "        [   5, 3651, 3173, 3873,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4357, 1103, 4312,    7, 2281,    3,    4,    0,    0],\n",
            "        [   5, 3865, 3232, 4261, 1922, 2215,    3,    4,    0,    0],\n",
            "        [   5, 3834, 3940, 3866, 4018,    3,    4,    0,    0,    0],\n",
            "        [   5, 1922, 3042, 2288, 4357,    3,    4,    0,    0,    0],\n",
            "        [   5, 1158, 3866, 1704,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 2833, 3786,    7,  315,    3,    4,    0,    0,    0],\n",
            "        [   5, 4357, 3030, 1366,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1037, 4357, 3344, 3869,    6,    4,    0,    0,    0],\n",
            "        [   5, 1922, 1113, 3781, 1771, 3865,    3,    4,    0,    0],\n",
            "        [   5, 4357, 3030, 1169,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 2303, 1890,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 4357, 2485, 1806, 1813,    3,    4,    0,    0,    0],\n",
            "        [   5, 2627, 3866,  455,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 3865, 3565, 3288,    3,    4,    0,    0,    0,    0],\n",
            "        [   5, 1922, 3273, 1836, 1629, 2654,    3,    4,    0,    0],\n",
            "        [   5, 2049, 3232, 3945, 2162,    3,    4,    0,    0,    0],\n",
            "        [   5, 3651, 2353, 1934, 4357,  570,    3,    4,    0,    0],\n",
            "        [   5, 4357, 3030, 3525, 2791,    3,    4,    0,    0,    0],\n",
            "        [   5, 1922, 2303,    7, 2483,    3,    4,    0,    0,    0],\n",
            "        [   5, 1776, 3232, 2124, 2602,  929,    3,    4,    0,    0],\n",
            "        [   5, 1103, 4357, 1603, 2049,    6,    4,    0,    0,    0],\n",
            "        [   5, 1922, 1771,    7, 3949,    3,    4,    0,    0,    0],\n",
            "        [   5, 2049, 3232,  103, 3360,    3,    4,    0,    0,    0],\n",
            "        [   5, 3940, 4168, 1488, 2353,    3,    4,    0,    0,    0],\n",
            "        [   5, 4357, 4238, 3333,    3,    4,    0,    0,    0,    0]])\n",
            "tensor([ 6,  6,  6,  7,  8,  6,  9,  6,  8,  8,  7,  6,  5,  6,  9,  9,  7,  9,\n",
            "         7,  6,  8,  6, 13,  6,  8,  8,  6,  8,  8,  7,  9,  6,  8,  8,  8,  6,\n",
            "         7,  7,  7,  7,  9,  8,  8,  6, 10,  6,  7,  8,  6,  7,  7,  6,  6,  8,\n",
            "         7,  8,  7,  6,  7,  8,  8,  7,  6,  7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VC--XvI9Lju"
      },
      "source": [
        "batch_size = 9\n",
        "def french_to_english(french_sentence):\n",
        "    french_tensor = [inp_lang.word2idx[w] for w in french_sentence.split()]\n",
        "    \n",
        "    lens = torch.tensor([len(french_tensor)] * batch_size)\n",
        "    lens = lens.to(device)\n",
        "\n",
        "    x = torch.tensor([list(french_tensor)] * batch_size)\n",
        "    x = x.to(device)\n",
        "\n",
        "    # xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "    enc_output, enc_hidden = encoder(x=x, lens=lens, device=device)\n",
        "    \n",
        "    # encode(french_sentence)\n",
        "    return enc_output"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD4ifOis9LeD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "8540d37b-784a-43f0-af58-50330b559ddb"
      },
      "source": [
        "french_to_english('<start> il ne peut pas vous aider . <end>')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-30bfbdb9e1f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfrench_to_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<start> il ne peut pas vous aider . <end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-e98be61c2e7d>\u001b[0m in \u001b[0;36mfrench_to_english\u001b[0;34m(french_sentence)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# xs, ys, lens = sort_batch(inp, targ, inp_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# encode(french_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-8f502332a23a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lens, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# x transformed = max_len X batch_size X embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# x = x.permute(1,0,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQdLX_6Zr-Ct"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}